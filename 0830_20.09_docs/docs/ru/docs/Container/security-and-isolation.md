

# Безопасность и изоляция

- [Безопасность и изоляция](#security-and-isolation)
  - [Пространство имен пользователей с отношениями many-to-many](#many-to-many-user-namespaces)
  - [Управление разрешениями пользователя](#user-permission-control)
  - [Изоляция файловой системы proc (Lxcfs)](#proc-file-system-isolation-(lxcfs))

## Пространство имен пользователей с отношениями many-to-many

### Описание функционала

Пространства имен пользователей используются для сопоставления пользователя контейнера с правами **root** с обычным пользователем хоста. С его помощью получают привилегии в контейнере те процессы и пользователи, которые не имеют привилегий на хосте. Этот механизм предотвращает атаки методом выхода из контейнера (container escape) на хост и выполнение несанкционированных операций. Кроме того, после использования пространства имен пользователей контейнер и хост используют разные идентификаторы UID и GID. Эта мера гарантирует, что такие пользовательские ресурсы в контейнере, как файловые дескрипторы, будут изолированы от ресурсов хоста.

Настройкой параметра **--user-remap** API в системных контейнерах пространства имен пользователей различных контейнеров сопоставляются с различными пространствами имен пользователей хоста и таким образом изолируются пространства имен пользователей контейнеров.

### Описание параметров

| Команда          | Параметр     | Описание значений                                            |
| ---------------- | ------------ | ------------------------------------------------------------ |
| isula create/run | --user-remap | Формат параметра: *uid**:**gid**:**offset*. Описание параметра:<br />  ·      Значения *uid* и *gid* должны быть целыми числами не меньше 0.<br />  ·      Значение *offset* должно быть целым числом больше 0 и меньше 65536. Значение не должно быть слишком маленьким. Иначе контейнер невозможно будет запустить.<br />  ·      Сумма значений *uid* и *offset* или же сумма значений *gid* и *offset* должна быть не больше 2<sup>32</sup> - 1. Иначе при запуске контейнера произойдет ошибка. |

### Ограничения

- Если в системном контейнере настроен параметр **--user-remap**, каталог rootfs должен быть доступен пользователям, заданным посредством идентификатора _uid_ или _gid_ в параметре **--user-remap**. В противном случае пространства имен пользователей контейнеров не смогут получить доступ к корневой файловой системе. В результате произойдет сбой запуска контейнеров.
- Все идентификаторы в контейнере можно сопоставить с каталогом rootfs хоста. Определенные каталоги или файлы можно смонтировать с хоста в контейнеры, например файлы устройств в каталоге **/dev/pts**. При очень маленьком значении _offset_ возможен сбой монтирования.
- Параметры _uid_, _gid_ и _offset_ управляются платформой планирования верхнего уровня. Контейнерный движок проверяет только допустимость их значений.
- Параметр **--user-remap** доступен только в системных контейнерах.
- Параметры **--user-remap** и **--privileged** одновременно не настраиваются. Иначе при запуске контейнера произойдет ошибка.
- Если *uid* или *gid* имеет значение **0**, параметр **--user-remap** не вступает в силу.

### Инструкция по использованию

> ![](./public_sys-resources/icon-note.gif) **ПРИМЕЧАНИЕ**:  
Перед настройкой параметра **--user-remap** сконфигурируйте значение сдвига (offset) для идентификаторов UID и GID всех каталогов и файлов в rootfs. Данное значение сдвига должно быть равно значению сдвига, установленному для *uid* и *gid* в параметре **--user-remap**.  
Например, выполните следующую команду, чтобы сдвинуть значения UID и GID всех файлов в каталоге **dev** на 100000:  
chown 100000:100000 dev

Укажите параметр **--user-remap** при запуске системного контейнера.

```
[root@localhost ~]# isula run -tid --user-remap 100000:100000:65535 --system-container --external-rootfs /home/root-fs none /sbin/init
eb9605b3b56dfae9e0b696a729d5e1805af900af6ce24428fde63f3b0a443f4a
```

Проверьте информацию о процессах /sbin/init на хосте и в контейнере.

```
[root@localhost ~]# isula exec eb ps aux | grep /sbin/init
root         1  0.6  0.0  21624  9624 ?        Ss   15:47   0:00 /sbin/init
[root@localhost ~]# ps aux | grep /sbin/init
100000    4861  0.5  0.0  21624  9624 ?        Ss   15:47   0:00 /sbin/init
root      4948  0.0  0.0 213032   808 pts/0    S+   15:48   0:00 grep --color=auto /sbin/init
```

Владельцем процесса /sbin/init в контейнере является пользователь **root**, а владельцем на хосте является пользователь, UID которого **100000**.

Создайте файл в контейнере и проверьте информацию о владельце файла на хосте.

```
[root@localhost ~]# isula exec -it eb bash
[root@localhost /]# echo test123 >> /test123
[root@localhost /]# exit
exit
[root@localhost ~]# ll /home/root-fs/test123
-rw-------. 1 100000 100000 8 Aug  2 15:52 /home/root-fs/test123
```

Владельцем файла, сгенерированного в контейнере, является пользователь **root**, а владельцем на хосте является пользователь, идентификатор которого **100000**.

## Управление разрешениями пользователя

### Описание функционала

Контейнерный движок поддерживает аутентификацию данных пользователей по протоколу TLS. Этот метод используется для управления разрешениями пользователя. В настоящее время функция управления разрешениями в контейнерных движках реализована путем подключения к плагину authz.

### Описание API

Плагин управления разрешениями указывается при настройке параметров запуска контейнерного движка iSulad. Конфигурация демона по умолчанию содержится в файле **/etc/isulad/daemon.json**.

| **Параметр**           | Пример                                  | Описание                                                     |
| ---------------------- | --------------------------------------- | ------------------------------------------------------------ |
| --authorization-plugin | "authorization-plugin":  "authz-broker" | Плагин аутентификации разрешений пользователя. В настоящее время поддерживается только плагин authz-broker. |

### Ограничения

- Для authz необходимо настроить политику выдачи разрешений пользователей. Политика по умолчанию содержится в файле **/var/lib/authz-broker/policy.json**. Этот файл можно динамически изменять, и изменение вступает в силу немедленно без перезапуска службы плагина.
- Контейнерный движок может запустить пользователь с правами **root**. Если обычным пользователям доступны для выполнения определенные команды, такие пользователи могут получить избыточные разрешения. Поэтому соблюдайте осторожность при выполнении данных операций. В настоящее время риск создают команды **container\_attach**, **container\_create**, **container\_exec\_create**.
- Некоторые комбинированные команды, например пара **isula exec** и **isula inspect** или пара **isula attach** и **isula inspect**, зависят от разрешения **isula inspect**. Если пользователь не имеет этого разрешения, передается ошибка.
- Использование каналов шифрования SSL/TLS усиливает безопасность, но при этом снижает производительность. Например, увеличивается задержка, потребляется больше ресурсов процессора, для выполнения операций шифрования и дешифрования требуется более высокая пропускная способность. Таким образом, количество параллельных операций уменьшается по сравнению с методом связи без аутентификации TLS. Согласно результатам тестирования, TLS используется для параллельного запуска контейнера в условиях, когда сервер ARM (Cortex-A72 64-core) почти не нагружен. Максимальное количество параллельных операций — от 200 до 250.
- Если на сервере указан параметр **--tlsverify**, путь по умолчанию, по которому сохраняются файлы аутентификации: **/etc/isulad**. Имена файлов по умолчанию: **ca.pem**, **cert.pem**, **key.pem**.

### Пример

1. Убедитесь, что на хосте установлен плагин authz. Если плагин authz не установлен, выполните следующую команду для установки и запуска службы плагина authz:
   
   ```
   [root@localhost ~]# yum install authz
   [root@localhost ~]# systemctl start authz
   ```

2. Чтобы включить эту функцию, настройте контейнерный движок и сертификат TLS. Для генерирования требуемого сертификата можно использовать OpenSSL.
   
   ```
   #SERVERSIDE
   
   # Generate CA key
   openssl genrsa -aes256 -passout "pass:$PASSWORD" -out "ca-key.pem" 4096
   # Generate CA
   openssl req -new -x509 -days $VALIDITY -key "ca-key.pem" -sha256 -out "ca.pem" -passin "pass:$PASSWORD" -subj "/C=$COUNTRY/ST=$STATE/L=$CITY/O=$ORGANIZATION/OU=$ORGANIZATIONAL_UNIT/CN=$COMMON_NAME/emailAddress=$EMAIL"
   # Generate Server key
   openssl genrsa -out "server-key.pem" 4096
   
   # Generate Server Certs.
   openssl req -subj "/CN=$COMMON_NAME" -sha256 -new -key "server-key.pem" -out server.csr
   
   echo "subjectAltName = DNS:localhost,IP:127.0.0.1" > extfile.cnf
   echo "extendedKeyUsage = serverAuth" >> extfile.cnf
   
   openssl x509 -req -days $VALIDITY -sha256 -in server.csr -passin "pass:$PASSWORD" -CA "ca.pem" -CAkey "ca-key.pem" -CAcreateserial -out "server-cert.pem" -extfile extfile.cnf
   
   #CLIENTSIDE
   
   openssl genrsa -out "key.pem" 4096
   openssl req -subj "/CN=$CLIENT_NAME" -new -key "key.pem" -out client.csr
   echo "extendedKeyUsage = clientAuth" > extfile.cnf
   openssl x509 -req -days $VALIDITY -sha256 -in client.csr -passin "pass:$PASSWORD" -CA "ca.pem" -CAkey "ca-key.pem" -CAcreateserial -out "cert.pem" -extfile extfile.cnf
   ```
   
   Если вы хотите использовать вышеприведенный контент в качестве скрипта, замените переменные настроенными значениями. Если параметр, используемый для генерирования CA-сертификата, не указан, установите ему значение **"**.  **PASSWORD**, **COMMON\_NAME**, **CLIENT\_NAME** **VALIDITY** — обязательные для установки параметры.

3. При запуске контейнерного движка добавьте параметры, связанные с TLS и плагином аутентификации, и убедитесь, что плагин корректно работает. Кроме того, для использования метода аутентификации TLS контейнерный движок должен быть запущен не в режиме сокета Unix, а в режиме прослушивания TCP. Настройка на демоне контейнера:
   
   ```
   {
       "tls": true,
       "tls-verify": true,
       "tls-config": {
              "CAFile": "/root/.iSulad/ca.pem",
              "CertFile": "/root/.iSulad/server-cert.pem",
              "KeyFile":"/root/.iSulad/server-key.pem"
       },
       "authorization-plugin": "authz-broker"
   }
   ```

4. Настройте политики. Для базового процесса авторизации все политики хранятся в конфигурационном файле **/var/lib/authz-broker/policy.json**. Файл можно динамически изменять без перезапуска плагина. При этом, процессу authz отправляется только сигнал SIGHUP. В данном файле одна строка содержит один объект политики JSON. Примеры настроенных политик:
   
   - Все пользователи могут выполнять все команды iSuald:  **{"name":"policy\_0","users":\[""],"actions":\[""]}**
   - Пользователь с именем Alice может выполнять все команды iSulad:  **{"name":"policy\_1","users":\["alice"],"actions":\[""]}**
   - Не указанный пользователь может выполнять все команды iSulad:  **{"name":"policy\_2","users":\[""],"actions":\[""]}**
   - Пользователи Alice и Bob могут создавать новые контейнеры:  **{"name":"policy\_3","users":\["alice","bob"],"actions":\["container\_create"]}**
   - Пользователь с именем service\_account может читать журналы и выполнять команду **docker top**:  **{"name":"policy\_4","users":\["service\_account"],"actions":\["container\_logs","container\_top"]}**
   - Пользователь с именем Alice может выполнять любые операции с контейнером:  **{"name":"policy\_5","users":\["alice"],"actions":\["container"]}**
   - Пользователь с именем Alice может выполнять любые операции с контейнером, но типом запроса может быть только **get**:  **{"name":"policy\_5","users":\["alice"],"actions":\["container"], "readonly":true}**
   
   > ![](./public_sys-resources/icon-note.gif) **ПРИМЕЧАНИЕ**:
   > 
   > - **action** означает, что поддерживаются регулярные выражения.
   > - **users** означает, что регулярные выражения не поддерживаются.
   > - Пользователи, указанные параметром **users**, должны быть уникальными. То есть к одному пользователю не применяются сразу несколько правил.

5. После обновления конфигурации настройте параметры TLS на клиенте для подключения к контейнерному движку. То есть необходимо установить ограничения на доступ к контейнерному движку в виде разрешений.
   
   ```
   [root@localhost ~]#  isula version --tlsverify --tlscacert=/root/.iSulad/ca.pem --tlscert=/root/.iSulad/cert.pem --tlskey=/root/.iSulad/key.pem  -H=tcp://127.0.0.1:2375
   ```
   
   Если планируется использовать аутентификацию TLS для подключения клиента по умолчанию, переместите конфигурационный файл в **~/.iSulad** и настройте переменные **ISULAD\_HOST** и **ISULAD\_TLS\_VERIFY** (вместо передачи параметров **-H=tcp://$HOST:2375** и -**-tlsverify** во время каждого вызова).
   
   ```
   [root@localhost ~]# mkdir -pv ~/.iSulad
   [root@localhost ~]# cp -v {ca,cert,key}.pem ~/.iSulad 
   [root@localhost ~]# export ISULAD_HOST=localhost:2375 ISULAD_TLS_VERIFY=1
   [root@localhost ~]# isula version
   ```

## Изоляция файловой системы proc (lxcfs)

### Сценарии применения

Виртуализация контейнеров — легко реализуемая, эффективная и быстро развертываемая технология. Однако контейнеры не являются строго изолированными, что неудобно для пользователей. Недостатки в изолировании контейнеров связаны с неидеальной организацией пространства имен в ядре Linux. Например, в файловой системе proc контейнера для просмотра доступна информация proc на хосте (такая как meminfo, cpuinfo, stat и uptime). Используя инструмент lxcfs, можно заменить содержимое каталога /proc экземпляров контейнера содержимым каталога /proc файловой системы хоста, чтобы службы в контейнере получали корректную информацию о ресурсах.

### Описание API

Системный контейнер предоставляет два пакета инструментов: lxcfs и lxcfs-toolkit, которые используются вместе. lxcfs размещается на хосте как процесс демона. lxcfs-toolkit монтирует файловую систему lxcfs хоста в контейнеры через механизм перехвата (hook).

Командная строка lxcfs-toolkit выглядит следующим образом:

```
lxcfs-toolkit [OPTIONS] COMMAND [COMMAND_OPTIONS]
```

| **Команда** | Функция                                                      | Параметр                                                     |
| ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| remount     | Повторно монтирует lxcfs в контейнеры.                       | **--all**: повторно монтирует lxcfs во все контейнеры. **--container-id**: повторно монтирует lxcfs в указанный контейнер. |
| umount      | Размонтирование lxcfs от контейнеров.                        | **--all**: размонтирует lxcfs ото всех контейнеров. <br />**--container-id**: размонтирует lxcfs от указанного контейнера. |
| check-lxcfs | Проверка корректности работы службы lxcfs.                   | --                                                           |
| prestart    | Монтирует каталог **/var/lib/lxcfs** в контейнер перед запуском службы lxcfs. | --                                                           |

### Ограничения

- В настоящее время в файловой системе proc поддерживаются только файлы **cpuinfo**, **meminfo**, **stat**, **diskstats**, **partitions**, **swaps**, **uptime**. Остальные файлы не изолируются от других файловых систем API ядра (например, sysfs).
- После установки пакета RPM генерируется образец файла JSON в **/var/lib/lcrd/hooks/hookspec.json**. Чтобы добавить функцию журнала, необходимо добавить конфигурацию **--log** во время настройки.
- Файл **diskstats** содержит только информацию о дисках, которые поддерживают планирование CFQ, вместо информации о разделах. Устройства в контейнерах отображаются в виде имен в каталоге **/dev**. Если имя устройства не существует, информация остается незаполненной. Кроме того, устройство, в котором находится корневой каталог контейнера, отображается как **sda**.
- При монтировании lxcfs требуется параметр **slave**. Если используется параметр **shared**, возможна утечка точки монтирования в контейнерах на хост, что негативно отражается на работе хоста.
- lxcfs поддерживает механизм постепенной деградации (graceful degradation) работы службы. В условиях сбоя в работе или недоступности службы lxcfs информация **cpuinfo**, **meminfo**, **stat**, **diskstats**, **partitions**, **swaps** и **uptime** в контейнерах касается только хоста, и остальные сервисные функции контейнеров не затрагиваются.
- Нижний уровень работы службы lxcfs зависит от модуля ядра FUSE и библиотеки libfuse. Поэтому ядру необходима поддержка FUSE.
- lxcfs поддерживает работу только 64-разрядных приложений в контейнерах. Если в контейнере работает 32-разрядное приложение, это может привести к тому, что считываемая приложением информация о процессоре (**cpuinfo**) не будет согласована.
- lxcfs имитирует представление ресурсов только для контрольных групп контейнеров (cgroups). Поэтому системным вызовам (например, sysconf) в контейнерах может быть доступна только информация о хосте. lxcfs не реализует изоляцию ядра.
- Информация о процессоре (cpuinfo), отображаемая после реализацией службой lxcfs механизма изоляции, имеет следующий вид:
  - **processor**: значение увеличивается с 0.
  - **physical id**: значение увеличивается с 0.
  - **sibliing**: имеет фиксированное значение 1.
  - **core id**: имеет фиксированное значение 0.
  - **cpu cores**: имеет фиксированное значение 1.

### Пример

1. Установите пакеты lxcfs и lxcfs-toolkit и запустите службу lxcfs.
   
   ```
   [root@localhost ~]# yum install lxcfs lxcfs-toolkit 
   [root@localhost ~]# systemctl start lxcfs
   ```

2. После запуска контейнера убедитесь, что в контейнере существует точка монтирования lxcfs.
   
   ```
   [root@localhost ~]# isula run -tid -v /var/lib/lxc:/var/lib/lxc --hook-spec /var/lib/isulad/hooks/hookspec.json --system-container --external-rootfs /home/root-fs none init
   a8acea9fea1337d9fd8270f41c1a3de5bceb77966e03751346576716eefa9782
   [root@localhost ~]# isula exec a8 mount | grep lxcfs
   lxcfs on /var/lib/lxc/lxcfs type fuse.lxcfs (rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other)
   lxcfs on /proc/cpuinfo type fuse.lxcfs (rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other)
   lxcfs on /proc/diskstats type fuse.lxcfs (rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other)
   lxcfs on /proc/meminfo type fuse.lxcfs (rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other)
   lxcfs on /proc/partitions type fuse.lxcfs (rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other)
   lxcfs on /proc/stat type fuse.lxcfs (rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other)
   lxcfs on /proc/swaps type fuse.lxcfs (rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other)
   lxcfs on /proc/uptime type fuse.lxcfs (rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other)
   ```

3. Выполните команду **update**, чтобы обновить настройки процессора и ресурсов памяти контейнера, и проверьте ресурсы контейнера. В следующем командном выводе в информации о ресурсах контейнера отображаются фактические данные ресурсов контейнера вместо данных хоста.
   
   ```
   [root@localhost ~]# isula update --cpuset-cpus 0-1 --memory 1G a8
   a8
   [root@localhost ~]# isula exec a8 cat /proc/cpuinfo
   processor       : 0
   BogoMIPS        : 100.00
   cpu MHz         : 2400.000
   Features        : fp asimd evtstrm aes pmull sha1 sha2 crc32 cpuid
   CPU implementer : 0x41
   CPU architecture: 8
   CPU variant     : 0x0
   CPU part        : 0xd08
   CPU revision    : 2
   
   processor       : 1
   BogoMIPS        : 100.00
   cpu MHz         : 2400.000
   Features        : fp asimd evtstrm aes pmull sha1 sha2 crc32 cpuid
   CPU implementer : 0x41
   CPU architecture: 8
   CPU variant     : 0x0
   CPU part        : 0xd08
   CPU revision    : 2
   
   [root@localhost ~]# isula exec a8 free -m
                 total        used        free      shared  buff/cache   available
   Mem:           1024          17         997           7           8        1006
   Swap:          4095           0        4095
   ```