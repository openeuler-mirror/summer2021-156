# Управление устройствами

- [Управление устройствами](#managing-devices)
  - [Настройка контроллера PCIe для виртуальной машины](#configuring-a-pcie-controller-for-a-vm)
  - [Управление виртуальными дисками](#managing-virtual-disks)
  - [Управление виртуальными сетевыми картами vNIC](#managing-vnics)
  - [Настройка виртуального последовательного порта](#configuring-a-virtual-serial-port)
  - [Управление сквозным доступом к устройствам](#managing-device-passthrough)
    - [Сквозной доступ PCI](#pci-passthrough)
    - [Сквозной доступ SR-IOV](#sr-iov-passthrough)
  - [Управление устройством USB виртуальной машины](#managing-vm-usb)
    - [Настройка контроллеров USB](#configuring-usb-controllers)
    - [Настройка устройства сквозного доступа USB](#configuring-a-usb-passthrough-device)
  - [Получение и сохранение мгновенных снимков](#storing-snapshots)

## Настройка контроллера PCIe для виртуальной машины

### Обзор

Сетевую плату NIC, контроллер диска и устройства сквозного доступа PCIe в виртуальной машине необходимо смонтировать на корневом порте PCIe. Каждый корневой порт соответствует слоту PCIe. Устройства, смонтированные на корневом порте, поддерживают горячую замену, но сам корневой порт не поддерживает горячую замену. Поэтому пользователям необходимо планировать максимальное количество корневых портов PCIe, резервируемых для виртуальной машины, с учетом горячей замены. Перед запуском виртуальной машины корневой порт настраивается в статическом режиме.

### Настройка корневого устройства PCIe, корневого порта PCIe и моста PCIe-PCI-Bridge

Контроллер PCIe виртуальной машины конфигурируется с помощью файла XML. Значением параметра **model** в файле XML для корневого устройства PCIe является **pcie-root**, для корневого порта PCIe — **pcie-root-port**, для PCIe-PCI-bridge — **pcie-to-pci-bridge**.

- Упрощенная настройка
  
  Добавьте в файл XML виртуальной машины следующее содержимое. Остальные атрибуты контроллера автоматически задаются службой libvirt.
  
  ```
    <controller type='pci' index='0' model='pcie-root'/>
    <controller type='pci' index='1' model='pcie-root-port'/>
    <controller type='pci' index='2' model='pcie-to-pci-bridge'/>
    <controller type='pci' index='3' model='pcie-root-port'/>
    <controller type='pci' index='4' model='pcie-root-port'/>
    <controller type='pci' index='5' model='pcie-root-port'/>
  ```
  
  Атрибуты **pcie-root** и **pcie-to-pci-most** занимают каждый один индекс (**index**). Таким образом, окончательное значение индекса равно количеству требуемых корневых портов + 1.

- Полная настройка
  
  Добавьте в файл XML виртуальной машины следующее содержимое:
  
  ```
    <controller type='pci' index='0' model='pcie-root'/>
    <controller type='pci' index='1' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='1' port='0x8'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x0' multifunction='on'/>
    </controller>
    <controller type='pci' index='2' model='pcie-to-pci-bridge'>
      <model name='pcie-pci-bridge'/>
      <address type='pci' domain='0x0000' bus='0x01' slot='0x00' function='0x0'/>
    </controller>
    <controller type='pci' index='3' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='3' port='0x9'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>
    </controller>
    <controller type='pci' index='3' model='pcie-root-port'>
  ```
  
  В данном содержимом:
  
  - Атрибуты **chassis** и **port** корневого порта должны указываться в возрастающем порядке. Поскольку атрибут PCIe-PCI-bridge стоит в середине, пропускается шасси под номером 2 (**chassis**), но номера портов (**port**) сохраняют порядок следования.
  - Атрибут функции адресов (**address function**) корневого порта принимает значения в диапазоне от **0\*0** до **0\*7**.
  - На каждом слоте можно подключить максимум восемь функций. При заполнении слота их количество увеличивается.
  
  Метод полной настройки сложен. Поэтому рекомендуется пользоваться упрощенным.

## Управление виртуальными дисками

### Обзор

Типы виртуальных дисков: virtio-blk, virtio-scsi и vhost-scsi. Диск virtio-blk имитирует устройство блочного типа, а virtio-scsi и vhost-scsi имитируют устройства SCSI.

- virtio-blk: может использоваться для общего системного диска и диска данных. В этой конфигурации виртуальный диск существует в виртуальной машине в виде **vd\[a-z]** или 
  **vd\[a-z]\[a-z]**.
- virtio-scsi: рекомендуется использовать для общего системного диска и диска данных. В этой конфигурации виртуальный диск существует в виртуальной машине в виде **sd\[a-z]** или 
  **sd\[a-z]\[a-z]**.
- vhost-scsi: рекомендуется использовать для виртуального диска, которому требуется высокая производительность. В этой конфигурации виртуальный диск существует в виртуальной машине в виде **sd\[a-z]** или **sd\[a-z]\[a-z]**.

### Процедура

Подробную информацию о настройке виртуального диска см. в разделе [3.2.4.1 Устройства хранения.](#storage-devices) В этом разделе описаны процедуры подключения и отключения виртуального диска на примере virtio-scsi.

- Подключение диска virtio-scsi.
  
  Выполните команду **virsh attach-device** для подключения виртуального диска virtio-scsi.
  
  ```
   # virsh attach-device <VMInstance> <attach-device.xml>
  ```
  
  Данной командой можно подключить диск к виртуальной машине в режиме онлайн. Информация о диске указывается в файле **attach-device.xml**. Ниже приведен пример файла **attach-device.xml**:
  
  ```
  ### attach-device.xml ###
      <disk type='file' device='disk'>
        <driver name='qemu' type='qcow2' cache='none' io='native'/>
        <source file='/path/to/another/qcow2-file'/>
        <backingStore/>
        <target dev='sdb' bus='scsi'/>
        <address type='drive' controller='0' bus='0' target='1' unit='0'/>
      </disk>
  ```
  
  Диск, подключенный путем выполнения вышеприведенной команды, станет недействительным после выключения или перезапуска виртуальной машины. Если виртуальный диск необходимо подключить к виртуальной машине на постоянной основе, выполните команду **virsh attach-device** с параметром **--config**.

- Отключение диска virtio-scsi.
  
  Если диск, подключенный в режиме онлайн, больше не используется, выполните команду **virsh detach**, чтобы динамически отключить его.
  
  ```
   # virsh detach-device <VMInstance> <detach-device.xml>
  ```
  
  В файле **detach-device.xml** указывается информация о диске, который требуется отключить. Данная информация должна совпадать с информацией, которая содержится в XML-файле во время динамического подключения.

## Управление виртуальными сетевыми картами vNIC

### Обзор

Типы виртуальных сетевых карт (vNIC): virtio-net, vhost-net и vhost-user. После создания виртуальной машины, возможно, потребуется подключить или отключить vNIC. openEuler поддерживает горячую замену NIC, в результате которой изменяется пропускная способность сети и повышается гибкость и масштабируемость системы.

### Процедура

Подробную информацию о настройке виртуальной NIC-карты см. в разделе [3.2.4.2 Сетевые устройства.](#network-device) В этом разделе описаны процедуры подключения и отключения виртуальной NIC-карты на примере vhost-net.

- Подключение виртуальной NIC-карты vhost-net.
  
  Выполните команду **virsh attach-device** для подключения виртуальной NIC-карты vhost-net.
  
  ```
   # virsh attach-device <VMInstance> <attach-device.xml>
  ```
  
  Данной командой можно подключить виртуальную NIC-карту vhost-net к работающей виртуальной машине. Информация о NIC указывается в файле **attach-device.xml**. Ниже приведен пример файла **attach-device.xml**:
  
  ```
  ### attach-device.xml ###
      <interface type='bridge'>
        <mac address='52:54:00:76:f2:bb'/>
        <source bridge='br0'/>
        <virtualport type='openvswitch'/>
        <model type='virtio'/>
        <driver name='vhost' queues='2'/>
      </interface>
  ```
  
  Виртуальная NIC-карта vhost-net, подключенная путем выполнения вышеприведенной команды, станет недействительной после выключения или перезапуска виртуальной машины. Если карту vNIC необходимо подключить к виртуальной машине на постоянной основе, выполните команду **virsh attach-device** с параметром **--config**.

- Отключение виртуальной NIC-карты vhost-net.
  
  Если виртуальная NIC-карта, подключенная в режиме онлайн, больше не используется, выполните команду **virsh detach**, чтобы динамически отключить ее.
  
  ```
   # virsh detach-device <VMInstance> <detach-device.xml>
  ```
  
  В файле **detach-device.xml** указывается информация о vNIC, которую требуется отключить. Данная информация должна совпадать с информацией, которая содержится в XML-файле во время динамического подключения.

## Настройка виртуального последовательного порта

### Обзор

Чтобы управлять системой и обеспечить работу служб, необходимо обеспечить связь между виртуальными машинами и хост-машинами в среде виртуализации. Однако в сложной сетевой архитектуре с облачной системой управления службы, функционирующие в плоскости управления, и виртуальные машины, функционирующие в сервисной плоскости, не могут взаимодействовать друг с другом на уровне 3. В результате нет оперативности в развертывании служб и сборе информации. Поэтому для связи между виртуальными и хост-машинами требуется виртуальный последовательный порт. Элементы конфигурации последовательного порта добавляются в конфигурационный файл XML виртуальной машины и, таким образом, становится возможным обмен данными между виртуальными и хост-машинами.

### Процедура

Консоль последовательного порта виртуальной машины Linux является псевдо-терминальным устройством, подключенным к хост-машине через последовательный порт ВМ. Консоль реализует интерактивные операции в виртуальной машине через хост-машину. В этом сценарии для последовательного порта необходимо настроить тип pty. В этом разделе описана процедура настройки последовательного порта pty.

- Добавьте следующие элементы конфигурации виртуального последовательного порта под узлом **devices** в конфигурационном файле XML виртуальной машины:
  
  ```
      <serial type='pty'>
      </serial>
      <console type='pty'>
        <target type='serial'/>
      </console>
  ```

- Выполните команду **virsh console**, чтобы подключиться к последовательному порту pty работающей виртуальной машины.
  
  ```
  # virsh console <VMInstance>
  ```

- Чтобы не пропустить ни одно сообщение последовательного порта, используйте параметр 
  **--console** для подключения к последовательному порту при запуске виртуальной машины.
  
  ```
  # virsh start --console <VMInstance>
  ```

## Управление сквозным доступом к устройствам

С помощью данной технологии виртуальные устройства получают доступ к физическим устройствам напрямую. Таким образом, повышается производительность ввода-вывода виртуальных машин.

В настоящее время используется метод сквозного доступа VFIO. В зависимости от типа устройства данным методом осуществляется сквозной доступ к PCI и к SR-IOV.

### Сквозной доступ к PCI

В данном методе на хосте для виртуальной машины назначается физическое устройство PCI. Виртуальная машина получает доступ к этому устройству напрямую. В методе сквозного доступа PCI используется сквозной доступ к устройствам VFIO. Конфигурационный файл в формате XML для реализации сквозного доступа PCI в виртуальной машине выглядит следующим образом:

```
<hostdev mode='subsystem' type='pci' managed='yes'>   
    <driver name='vfio'/> 
    <source>
        <address domain='0x0000' bus='0x04' slot='0x10' function='0x01'/>
    </source>
    <rom bar='off'/>
    <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
</hostdev>
```

**Табл. 1** Элементы конфигурации устройства для реализации сквозного доступа PCI

| Параметр                        | Описание                                                     | Значение                                                     |
| ------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| hostdev.source.address.domain   | Идентификатор домена устройства PCI в операционной системе хоста. | ≥ 0                                                          |
| hostdev.source.address.bus      | Идентификатор шины устройства PCI в операционной системе хоста. | ≥ 1                                                          |
| hostdev.source.address.slot     | Идентификатор устройства PCI в операционной системе хоста.   | ≥ 0                                                          |
| hostdev.source.address.function | Идентификатор функции устройства PCI в операционной системе хоста. | ≥ 0                                                          |
| hostdev.driver.name             | Драйвер на стороне сервера для сквозного доступа PCI. Параметр необязателен. | **vfio** (значение по умолчанию)                             |
| hostdev.rom                     | Параметр определяет, может виртуальная машина получить доступ к ROM устройства со сквозным доступом или не может. | Параметр принимает значения **on** или **off**. Значение по умолчанию — **on**. **on**: виртуальная машина может получить доступ к ROM устройства со сквозным доступом. Например, если виртуальной машине с картой NIC со сквозным доступом необходимо загрузиться из среды выполнения предзагрузки (PXE), или  виртуальной машине с картой HBA со сквозным доступом необходимо загрузиться из ROM, этот параметр устанавливается в значение **on**. **off**: виртуальная машина не может получить доступ к ROM устройства со сквозным доступом. |
| hostdev.address type            | Идентификаторы шины, устройства и функции (BDF) в гостевой ОС, отображаемые в устройстве PCI. | [0x03–0x1e] (диапазон значений — идентификаторы слотов) <br />Примечание:<br />  ·   **Domain** — информация о домене, **bus** — идентификатор шины, **slot** — идентификатор слота, **function** — функция.<br />·   За исключением **slot**, значения данных параметров по умолчанию равны **0**. <br /> ·   Первый слот **0x00** отведен под систему, второй слот **0x01** — под контроллеры IDE и USB, третий слот **0x02** — под видео.<br />  ·   Последний слот **0x1f** занят каналом PV. |



> ![](./public_sys-resources/icon-note.gif) **ПРИМЕЧАНИЕ**:  
Сквозной доступ VFIO реализуется группой IOMMU. Устройства разделяются на группы IOMMU в соответствии с правилами службы контроля доступа (Access Control Service; ACS) к аппаратному обеспечению. Устройства одной группы IOMMU можно назначить только одной виртуальной машине. Если несколько функций в устройстве PCI принадлежат одной и той же группе IOMMU, их также можно назначить непосредственно только одной виртуальной машине.

### Сквозной доступ SR-IOV

#### Обзор

Виртуализация ввода-вывода с единым корнем (Single Root I/O Virtualization; SR-IOV) представляет собой одно из решений аппаратной виртуализации. С помощью технологии SR-IOV физическая функция (Physical Function; PF) предоставляет несколько виртуальных функций (Virtual Function; VF), и каждую VF можно напрямую назначить виртуальной машине. Это значительно улучшает эффективность использования аппаратных ресурсов и производительность ввода-вывода виртуальных машин. Для карт NIC, как правило, применяется метод сквозного доступа SR-IOV. С помощью технологии SR-IOV физическая карта NIC может функционировать как несколько виртуальных NIC (с виртуальными функциями VF), и каждую VF можно напрямую назначить виртуальной машине.

> ![](./public_sys-resources/icon-note.gif) **ПРИМЕЧАНИЕ**:
> 
> - Для реализации SR-IOV требуется соответствующая поддержка физического аппаратного обеспечения. Перед использованием SR-IOV убедитесь, что напрямую назначаемое аппаратное устройство поддерживает технологию SR-IOV, а драйвер устройства в ОС хоста работает в режиме SR-IOV.
> - Далее описывается процедура запроса модели NIC.  
В приведенном командном выводе значения в первом столбце — это номера PCI карт NIC, а **19e5:1822** — идентификатор поставщика и идентификатор карты NIC.

> ```
> # lspci | grep Ether  
> 05:00.0 Ethernet controller: Device 19e5:1822 (rev 45)  
> 07:00.0 Ethernet controller: Device 19e5:1822 (rev 45)  
> 09:00.0 Ethernet controller: Device 19e5:1822 (rev 45)  
> 0b:00.0 Ethernet controller: Device 19e5:1822 (rev 45)  
> 81:00.0 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01)  
> 81:00.1 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01)  
> ```

> ```
> 
> ```

#### Процедура

Чтобы настроить для карты NIC сквозной доступ методом SR-IOV, выполните следующие шаги:

1. Включение режима SR-IOV для NIC.
   
   1. Убедитесь, что гостевая ОС поддерживает драйверы VF, предоставленные поставщиком NIC. В противном случае VF в гостевой ОС не смогут работать корректно.
   
   2. Включите поддержку SMMU/IOMMU в BIOS ОС хоста. Метод включения для серверов разных поставщиков будет отличаться. Подробную информацию см. в справочной документации на серверы.
   
   3. Настройте драйвер хоста для включения режима SR-IOV VF. Ниже описывается способ включения 16 VF на примере NIC Hi1822.
      
      ```
      echo 16 > /sys/class/net/ethX/device/sriov_numvfs
      ```

2. Получение информации PCI BDF физической и виртуальной карт (PF и VF).
   
   1. Чтобы получить перечень ресурсов NIC, выполните следующую команду:
      
      ```
      # lspci | grep Eth
      03:00.0 Ethernet controller: Huawei Technologies Co., Ltd. Hi1822 Family (4*25GE) (rev 45)
      04:00.0 Ethernet controller: Huawei Technologies Co., Ltd. Hi1822 Family (4*25GE) (rev 45)
      05:00.0 Ethernet controller: Huawei Technologies Co., Ltd. Hi1822 Family (4*25GE) (rev 45)
      06:00.0 Ethernet controller: Huawei Technologies Co., Ltd. Hi1822 Family (4*25GE) (rev 45)
      7d:00.0 Ethernet controller: Huawei Technologies Co., Ltd. Device a222 (rev 20)
      7d:00.1 Ethernet controller: Huawei Technologies Co., Ltd. Device a222 (rev 20)
      7d:00.2 Ethernet controller: Huawei Technologies Co., Ltd. Device a221 (rev 20)
      7d:00.3 Ethernet controller: Huawei Technologies Co., Ltd. Device a221 (rev 20)
      ```
   
   2. Проверьте информацию о PCI BDF виртуальной карты следующей командой:
      
      ```
      # lspci | grep "Virtual Function"
      03:00.1 Ethernet controller: Huawei Technologies Co., Ltd. Hi1822 Family Virtual Function (rev 45)
      03:00.2 Ethernet controller: Huawei Technologies Co., Ltd. Hi1822 Family Virtual Function (rev 45)
      03:00.3 Ethernet controller: Huawei Technologies Co., Ltd. Hi1822 Family Virtual Function (rev 45)
      03:00.4 Ethernet controller: Huawei Technologies Co., Ltd. Hi1822 Family Virtual Function (rev 45)
      03:00.5 Ethernet controller: Huawei Technologies Co., Ltd. Hi1822 Family Virtual Function (rev 45)
      03:00.6 Ethernet controller: Huawei Technologies Co., Ltd. Hi1822 Family Virtual Function (rev 45)
      03:00.7 Ethernet controller: Huawei Technologies Co., Ltd. Hi1822 Family Virtual Function (rev 45)
      03:01.0 Ethernet controller: Huawei Technologies Co., Ltd. Hi1822 Family Virtual Function (rev 45)
      03:01.1 Ethernet controller: Huawei Technologies Co., Ltd. Hi1822 Family Virtual Function (rev 45)
      03:01.2 Ethernet controller: Huawei Technologies Co., Ltd. Hi1822 Family Virtual Function (rev 45)
      ```
   
   3. Выберите доступную VF и внесите ее конфигурацию в конфигурационный файл виртуальной машины в соответствии с информацией BDF. Например, идентификатором шины устройства **03:00.1** является **03**, идентификатором слота — **00**, идентификатором его функции — **1**.

3. Идентификация и настройка соответствия между PF и VF.
   
   1. Идентификация устройств VF, соответствующих определенному устройству PF. Далее приведена команда на примере **PF 03.00.0**:
      
      ```
      # ls -l /sys/bus/pci/devices/0000\:03\:00.0/
      ```
      
      На экране появится следующая информация о символической ссылке. На ее основе можно получить идентификаторы VF (virtfnX) и PCI BDF.
   
   2. Идентификация устройства PF, соответствующего определенному устройству VF. Далее приведена команда на примере **VF 03.00.1**:
      
      ```
      # ls -l /sys/bus/pci/devices/0000\:03\:00.1/
      ```
      
      На экране появится следующая информация о символической ссылке. На ее основе можно получить идентификаторы PCI BDF данного устройства PF.
      
      ```
      lrwxrwxrwx 1 root root       0 Mar 28 22:44 physfn -> ../0000:03:00.0
      ```
   
   3. Получите имена карт NIC, соответствующих данным PF или VF. Пример:
      
      ```
      # ls /sys/bus/pci/devices/0000:03:00.0/net
      eth0
      ```
   
   4. Настройте MAC-адрес, VLAN и информацию QoS устройства VF, которые должны быть включены (состояние **Up**) для осуществления сквозного доступа. Далее приведена команда на примере **VF 03.00.1**. PF — eth0, а VF ID — 0.
      
      ```
      # ip link set eth0 vf 0 mac 90:E2:BA:21:XX:XX    #Sets the MAC address.
      # ifconfig eth0 up
      # ip link set eth0 vf 0 rate 100                 #Sets the VF outbound rate, in Mbit/s.
      # ip link show eth0                              #Views the MAC address, VLAN ID, and QoS information to check whether the configuration is successful.
      ```

4. Подключение NIC SR-IOV к виртуальной машине.
   
   При создании виртуальной машины добавьте элемент конфигурации сквозного доступа SR-IOV в конфигурационный файл виртуальной машины.
   
   ```
   <interface type='hostdev' managed='yes'> 
       <mac address='fa:16:3e:0a:xx:xx'/>
       <source> 
           <address type='pci' domain='0x0000' bus='0x06' slot='0x11' function='0x6'/>
       </source> 
       <vlan>
           <tag id='1'/>
       </vlan>
   </interface>
   ```
   
   **Табл. 1** Параметры настройки SR-IOV
   
   | **Параметр**                    | Описание                                                     | Значение                                                     |
   | ------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
   | hostdev.managed                 | Два режима обработки службой libvirt процессов устройств PCI. | **no**: значение по умолчанию. Устройство сквозного доступа управляется пользователем. **yes**: устройство сквозного доступа управляется службой libvirt. В сценарии сквозного доступа SR-IOV установите этому параметру значение **yes**. |
   | hostdev.source.address.bus      | Идентификатор шины устройства PCI в операционной системе хоста. | ≥ 1                                                          |
   | hostdev.source.address.slot     | Идентификатор устройства PCI в операционной системе хоста.   | ≥ 0                                                          |
   | hostdev.source.address.function | Идентификатор функции устройства PCI в операционной системе хоста. | ≥ 0                                                          |
   
   
   
   > ![](./public_sys-resources/icon-note.gif) **ПРИМЕЧАНИЕ**:  
   Отключение функции SR-IOV:  
   Чтобы отключить функцию SR-IOV после остановки виртуальной машины при условии, что ни одно устройство VF не используется, выполните следующую команду.  
   Далее приведена команда на примере NIC Hi1822 (имя соответствующего сетевого интерфейса: eth0):
   > 
   > ```
   > echo 0 > /sys/class/net/eth0/device/sriov_numvfs  
   > ```

#### Сквозной доступ SR-IOV для ускорителя HPRE

Данный движок используется в качестве решения аппаратного ускорения, предоставляемого сервером TaiShan 200 на базе процессора Kunpeng 920. Ускоритель HPRE используется для ускорения работы приложений SSL/TLS. Движок значительно снижает загрузку процессоров и повышает эффективность использования их ресурсов.  
Для внутренних служб виртуальной машины на сервере Kunpeng необходимо выполнить проброс VF ускорителя HPRE хоста в виртуальную машину.

**Табл. 1** Описание ускорителя HPRE

| Параметр                    | Описание                                                     |
| --------------------------- | ------------------------------------------------------------ |
| Имя устройства              | Встроенный в чип Hi1620 ускоритель криптографического алгоритма на основе протокола RSA/DH (движок HPRE) |
| Функции                     | Возведение в степень по модулю, операции с парой ключей по алгоритму RSA, вычисление ключа по алгоритму DH и вспомогательные операции с большими числами (возведение в степень по модулю, умножение по модулю, деление с остатком, вычисление обратного мультипликативного элемента по модулю, выполнение теста на примитивность и теста на взаимную примитивность). |
| Идентификатор поставщика    | 0x19E5                                                       |
| Идентификатор устройства PF | 0xA258                                                       |
| Идентификатор устройства VF | 0xA259                                                       |
| Максимальное количество VF  | Для одного физического устройства HPRE PF можно создать максимум 63 устройства VF. |


> ![](./public_sys-resources/icon-note.gif) **ПРИМЕЧАНИЕ**:  
Если виртуальная машина использует устройство VF, драйвер на хосте нельзя удалить, и ускоритель в этом случае не поддерживает горячую замену.  
Операция с устройством VF (если VFNUMS равно 0, VF отключено, hpre\_num используется для идентификации определенного устройства ускорителя):
> 
> ```
> echo $VFNUMS > /sys/class/uacce/hisi_hpre-$hpre_num/device/sriov_numvfs
> ```

## Управление устройством USB виртуальной машины

Для использования таких USB-устройств, как USB-устройства ключей и USB-устройства массового хранения на виртуальных машинах, openEuler предоставляет функцию сквозного доступа к устройствам USB. Через интерфейсы сквозного доступа USB и интерфейсы горячей замены настраиваются устройства сквозного доступа USB для виртуальных машин или устройства USB горячей замены в процессе работы виртуальных машин.

### Настройка контроллеров USB

#### Обзор

Контроллер USB — это виртуальный контроллер, который предоставляет ряд функций USB для USB-устройств в виртуальных машинах. Чтобы использовать USB-устройства в виртуальной машине, необходимо настроить для нее контроллеры USB. В настоящее время openEuler поддерживает следующие типы контроллеров USB:

- Универсальный интерфейс контроллера хоста (Universal Host Controller Interface; UHCI), также распространенный под названием USB 1.1.
- Усовершенствованный интерфейс контроллера хоста (Enhanced Host Controller Interface; EHCI), также распространенный под названием USB 2.0.
- Расширяемый интерфейс контроллера хоста (Extensible Host Controller Interface; EHCI), также распространенный под названием USB 3.0.

#### Меры предосторожности

- Хост-сервер должен иметь аппаратное обеспечение контроллера USB и модули, которые поддерживают спецификации USB 1.1, USB 2.0 и USB 3.0.
- Контроллеры USB для виртуальной машины необходимо настраивать в следующем порядке: USB 1.1, USB 2.0 и USB 3.0.
- Контроллер xHCI имеет восемь портов, и его можно смонтировать максимум с четырьмя устройствами USB 3.0 и четырьмя устройствами USB 2.0. Контроллер EHCI имеет шесть портов, и его можно смонтировать максимум с шестью устройствами USB 2.0. Контроллер UHCI имеет два порта, и его можно смонтировать максимум с двумя устройствами USB 1.1.
- На каждой виртуальной машине можно настроить только один контроллер USB такого же типа.
- Контроллеры USB не поддерживают замену в горячем режиме.
- Если в виртуальной машине не установлен драйвер USB 3.0, есть вероятность, что контроллер xHCI не будет идентифицирован. Подробную информацию о загрузке и установке драйвера USB 3.0 см. в официальном описании, представленном дистрибьютором операционной системы.
- Чтобы обеспечить совместимость ОС, установите идентификатору шины контроллера USB значение **0** при настройке планшета с USB для виртуальной машины. Планшет по умолчанию монтируется на контроллер USB 1.1.

#### Методы настройки

Ниже описываются элементы конфигурации контроллеров USB для виртуальной машины. Рекомендуется сконфигурировать USB 1.1, USB 2.0 и USB 3.0, чтобы обеспечить совместимость виртуальной машины с устройств данных трех типов.

Элемент конфигурации контроллера USB 1.1 (UHCI) в конфигурационном файле XML выглядит следующим образом:

```
<controller type='usb' index='0' model='piix3-uhci'>
</controller>
```

Элемент конфигурации контроллера USB 2.0 (EHCI) в конфигурационном файле XML выглядит следующим образом:

```
<controller type='usb' index='1' model='ehci'>
</controller>
```

Элемент конфигурации контроллера USB 3.0 (xHCI) в конфигурационном файле XML выглядит следующим образом:

```
<controller type='usb' index='2' model='nec-xhci'>
</controller>
```

### Настройка устройства сквозного доступа USB

#### Обзор

После настройки контроллеров USB для виртуальной машины в нее можно смонтировать физическое USB-устройство хоста методом проброса для дальнейшего использования виртуальной машины. В сценарии виртуализации, помимо статической конфигурации, поддерживается горячая замена USB-устройства. То есть USB-устройство можно монтировать и размонтировать в процессе работы виртуальной машины.

#### Меры предосторожности

- Устройства USB можно назначить только одной виртуальной машине.
- Виртуальная машина с устройством сквозного доступа USB не поддерживает миграцию в реальном времени.
- Создание виртуальной машины невозможно, если в конфигурационном файле виртуальной машины не указаны устройства сквозного доступа USB.
- Принудительное удаление в горячем режиме USB-диска, который в данный момент выполняет операции чтения или записи, может привести к повреждению файлов, хранящихся на данном диске.

#### Описание конфигурации

Ниже описываются элементы конфигурации устройства USB для виртуальной машины.

Описание устройства USB в конфигурационном файле XML:

```
<hostdev mode='subsystem' type='usb' managed='yes'>
    <source>        
        <address bus='m' device='n'/>
    </source>
    <address type='usb' bus='x' port='y'/>
</hostdev>
```

- **\<address bus='**_m_**'device='**_n_**'/>**: *m* — адрес шины USB на хосте, а *n* — идентификатор устройства.
- **\<address type='usb'bus='**_x_**'port='**_y_**'>**: означает, что USB-устройство должно быть смонтировано на контроллер USB, указанный в виртуальной машине. *x* — идентификатор контроллера, который соответствует номеру индекса контроллера USB, настроенного в виртуальной машине. _y_ — адрес порта. Настраивая устройство сквозного доступа USB, необходимо установить этот параметр, чтобы обеспечить требуемый контроллер, на который монтируется устройство.

#### Методы настройки

Для настройки сквозного доступа выполните следующие действия:

1. Настройте контроллеры USB для виртуальной машины. Для получения подробной информации обратитесь к разделу [Настройка контроллеров USB](#configuring-usb-controllers).

2. Запросите информацию об устройстве USB на хосте.
   
   Выполните команду **lsusb** (требуется установить пакет программного обеспечения **usbutils**) для запроса информации об устройстве USB на хосте, в том числе, адрес шины, адрес устройства, идентификатор поставщика устройства, идентификатор устройства и описание продукта. Пример:
   
   ```
   # lsusb
   ```
   
   ```
   Bus 008 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub
   Bus 007 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
   Bus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
   Bus 004 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
   Bus 006 Device 002: ID 0bda:0411 Realtek Semiconductor Corp. 
   Bus 006 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub
   Bus 005 Device 003: ID 136b:0003 STEC 
   Bus 005 Device 002: ID 0bda:5411 Realtek Semiconductor Corp. 
   Bus 005 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
   Bus 001 Device 003: ID 12d1:0003 Huawei Technologies Co., Ltd. 
   Bus 001 Device 002: ID 0bda:5411 Realtek Semiconductor Corp. 
   Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
   Bus 003 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
   ```

3. Подготовьте XML-файл с описанием данного устройства USB. Перед тем, как извлечь устройство в горячем режиме, убедитесь, что оно не используется в данный момент. В противном случае возможна потеря данных.

4. Выполните команды горячей замены.
   
   Далее команды приведены на примере виртуальной машины с именем **openEulerVM**. Соответствующий конфигурационный файл **usb.xml**.
   
   - Операция добавления устройства USB в горячем режиме действительна только для текущей запущенной виртуальной машины. После перезапуска виртуальной машины еще раз добавьте устройство USB в горячем режиме.
     
     ```
     # virsh attach-device openEulerVM usb.xml --live
     ```
   
   - Настройте атрибут постоянства (persistency) для горячего добавления устройства USB. После перезапуска виртуальной машины устройство USB автоматически назначается данной ВМ.
     
     ```
     # virsh attach-device openEulerVM usb.xml --config
     ```
   
   - Операция удаления устройства USB в горячем режиме действительна только для текущей запущенной виртуальной машины. После перезапуска виртуальной машины устройство USB с настроенным атрибутом постоянства автоматически назначается данной ВМ.
     
     ```
     # virsh detach-device openEulerVM usb.xml --live
     ```
   
   - Настройте атрибут постоянства (persistency) для горячего удаления устройства USB.
     
     ```
     # virsh detach-device openEulerVM usb.xml --config
     ```

## Получение и сохранение мгновенных снимков

### Обзор

Система виртуальной машины может быть повреждена вирусом или в результате ошибочного удаления системного файла, или неправильного форматирования. Поврежденную систему невозможно запустить. Чтобы быстро восстановить поврежденную систему, openEuler предоставляет функцию мгновенного снимка состояния системы. Такой мгновенный снимок записывает состояние виртуальной машины в указанное время, и для пользователей эта операция незаметна (обычно занимает несколько секунд). Мгновенный снимок можно использовать для восстановления виртуальной машины в состояние, которое было на момент получения снимков. Данный метод повышает надежность системы.

> ![](./public_sys-resources/icon-note.gif) **ПРИМЕЧАНИЕ**:  
В настоящее время поддерживаются мгновенные снимки образов QCOW2 и RAW. Блочные устройства не поддерживаются.

### Процедура

Для создания мгновенного снимка состояния виртуальной машины выполните следующие действия:

1. Войдите в хост и выполните команду **virsh domblklist** для запроса диска, используемого виртуальной машиной.
   
   ```
   # virsh domblklist openEulerVM
     Target   Source
    ---------------------------------------------
     vda      /mnt/openEuler-image.qcow2
   ```

2. Чтобы создать мгновенный снимок диска виртуальной машины **openEuler-snapshot1.qcow2**, выполните следующую команду:
   
   ```
   # virsh snapshot-create-as --domain openEulerVM --disk-only --diskspec vda,snapshot=external,file=/mnt/openEuler-snapshot1.qcow2 --atomic
   Domain snapshot 1582605802 created
   ```

3. Проверьте мгновенные снимки диска следующей командой:
   
   ```
   # virsh snapshot-list openEulerVM
    Name         Creation Time               State
   ---------------------------------------------------------
    1582605802   2020-02-25 12:43:22 +0800   disk-snapshot
   ```