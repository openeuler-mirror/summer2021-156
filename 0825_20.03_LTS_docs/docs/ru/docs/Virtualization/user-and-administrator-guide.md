# Руководство пользователя и администратора

В этой главе описывается, как создавать виртуальные машины (ВМ) на платформе виртуализации, управлять жизненными циклами ВМ и запрашивать информацию.

- [Передовые методы](#best-practices)
  - [[Передовые методы](#best-practices) контроля производительности](#performance-best-practices)
    - [Опрос в режиме ожидания](#halt-polling)
    - [Настройка атрибутов потока ввода-вывода](#i-o-thread-configuration)
    - [Raw Device Mapping](#raw-device-mapping)
    - [Изоляция и привязка процесса kworker](#kworker-isolation-and-binding)
    - [Подкачка памяти методом HugePage](#hugepage-memory)
  - [[Передовые методы](#best-practices) обеспечения безопасности](#security-best-practices)
    - [Аутентификация libvirt](#libvirt-authentication)
    - [qemu-ga](#qemu-ga)
    - [Защита sVirt](#svirt-protection)

## Передовые методы

### Передовые методы контроля производительности

#### Опрос в режиме ожидания

##### Обзор

Если вычислительных ресурсов достаточно, функция опроса в режиме ожидания применяется в целях доведения производительности виртуальных машин до уровня физических машин. Если функция опроса в режиме ожидания не включена, то когда виртуальный процессор (vCPU) завершает работу по причине таймаута ожидания, хост выделяет ресурсы физического процессора другим процессам. Если данная функция включена на хосте, vCPU виртуальной машины, перейдя в режим ожидания, выполняет опрос. Продолжительность опроса зависит от фактической конфигурации. Перейдя в активное состояние во время опроса, vCPU может продолжить работу без планирования с хоста. Это уменьшает потребление ресурсов на планирование и улучшает производительность системы виртуальной машины.

> ![](./public_sys-resources/icon-note.gif) ПРИМЕЧАНИЕ:  
Механизм опроса в режиме ожидания гарантирует своевременный отклик потока vCPU виртуальной машины. Однако в отсутствии нагрузки виртуальной машины хост также выполняет опрос. В результате, хост обнаруживает большое потребление физических ресурсов процессора данного vCPU, но фактическая загрузка процессора ВМ не высокая.

##### Инструкции

Функция опроса в режиме ожидания включена по умолчанию. Можно динамически изменить время опроса в режиме ожидания vCPU, внеся изменения в файл **halt\_poll\_ns**. Значение по умолчанию — **500000** (нс).

Например, чтобы задать длительности опроса значение 400 000 нс, необходимо выполнить следующую команду:

```
# echo 400000 > /sys/module/kvm/parameters/halt_poll_ns
```

#### Настройка атрибутов потока ввода-вывода

##### Обзор

По умолчанию главные потоки QEMU обрабатывают операции чтения и записи виртуальной машины на стороне сервера в KVM. Это вызывает следующие проблемы:

- Запросы ввода-вывода ВМ обрабатываются главным потоком QEMU. Таким образом, однопоточные процессоры становятся узким местом производительности операций ввода-вывода в виртуальной машине.
- Когда запросы ввода-вывода в ВМ обрабатываются главным потоком QEMU, применяется глобальная блокировка QEMU (qemu\_global\_mutex). Продолжительное время обработки операций ввода-вывода увеличивает длительность использования глобальной блокировки главным потоком QEMU. В результате, невозможно оптимально запланировать ресурсы vCPU виртуальной машины, что влияет на ее общую производительность и отражается на пользовательском опыте.

Пользователь может сконфигурировать атрибут потока ввода-вывода для диска virtio-blk или контроллера virtio-scsi. На серверной стороне QEMU поток ввода-вывода используется для обработки запросов на чтение и запись виртуального диска. Взаимно однозначное соответствие, установленное между потоком ввода-вывода и диском virtio-blk или контроллером virtio-scsi, сведет к минимуму воздействие на главный поток QEMU, повысит общую производительность ввода-вывода виртуальной машины и улучшит пользовательский опыт.

##### Описание конфигурации

Чтобы использовать потоки ввода-вывода для обработки запросов на чтение и запись дисков виртуальной машины, необходимо изменить конфигурацию ВМ следующим образом:

- Настройте общее количество высокопроизводительных виртуальных дисков в виртуальной машине. Например, для изменения общего количества потоков ввода-вывода задайте параметру **\<iothreads>** значение **4**.
  
  ```
  <domain type='kvm' xmlns:qemu='http://libvirt.org/schemas/domain/qemu/1.0'>   
       <name>VMName</name>
       <memory>4194304</memory>
       <currentMemory>4194304</currentMemory>
       <vcpu>4</vcpu>
       <iothreads>4</iothreads>
  ```

- Настройте атрибут потока ввода-вывода для диска virtio-blk. **\<iothread>** отражает идентификаторы потоков ввода-вывода. Значения идентификаторов начинаются с 1, и каждый идентификатор должен быть уникальным. Максимальное значение идентификатора и есть значение параметра **\<iothreads>**. Например, для распределения потока ввода-вывода 2 на диск virtio-blk установите параметры следующим образом:
  
  ```
  <disk type='file' device='disk'>
        <driver name='qemu' type='raw' cache='none' io='native' iothread='2'/>
        <source file='/path/test.raw'/>
        <target dev='vdb' bus='virtio'/>
        <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
  </disk>
  ```

- Настройте атрибут потока ввода-вывода для контроллера virtio-scsi. Например, для распределения потока ввода-вывода 2 на контроллер virtio-scsi установите параметры следующим образом:
  
  ```
  <controller type='scsi' index='0' model='virtio-scsi'>
        <driver iothread='2'/>
        <alias name='scsi0'/>
        <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
  </controller>
  ```

- Привяжите потоки ввода-вывода к физическому процессору.
  
  Привязка потоков ввода-вывода к указанным физическим процессорам не отражается на загрузке ресурсов потоков vCPU. Параметр **\<iothread>** задает идентификаторы потоков ввода-вывода, параметр **\<cpuset>** задает идентификаторы привязанных физических процессоров.
  
  ```
  <cputune>
  <iothreadpin iothread='1' cpuset='1-3,5,7-12' />
  <iothreadpin iothread='2' cpuset='1-3,5,7-12' />
  </cputune>
  ```

#### Raw Device Mapping

##### Обзор

При настройке устройств хранения виртуальной машины можно использовать конфигурационные файлы. Таким образом настраиваются виртуальные диски для ВМ или к виртуальным машинам подключаются блочные устройства (например, физические LUNы ​и LV) для повышения производительности хранилища. Последний метод настройки называется прямым доступом виртуальной машины к конкретному LUNу устройства хранения (Raw Device Mapping; RDM). С помощью механизма RDM виртуальный диск предоставляется виртуальной машине в качестве устройства с интерфейсом физического подключения и передачи данных между компьютерами и периферийными устройствами (SCSI) и поддерживает большинство команд SCSI.

В зависимости от реализации на серверной стороне, выделяются два типа RDM-диска: виртуальный диск RDM и физический диск RDM. По сравнению с виртуальным RDM, физический RDM более производителен и поддерживает больше команд SCSI. Однако чтобы использовать физический RDM, весь диск SCSI необходимо смонтировать в виртуальную машину. Если во время настройки выделяются разделы или логические тома, виртуальная машина не сможет идентифицировать данный диск.

##### Пример настройки

Для RDM необходимо изменить конфигурационные файлы виртуальной машины. Примеры настройки:

- Виртуальный диск RDM
  
  Далее приведен пример монтирования диска SCSI **/dev/sdc** на хосте для предоставления его виртуальной машине в качестве виртуального неформатированного устройства:
  
  ```
  <domain type='kvm'>
   <devices>
      ...
      <controller type='scsi' model='virtio-scsi' index='0'/>
      <disk type='block' device='disk'>
          <driver name='qemu' type='raw' cache='none' io='native'/>
          <source dev='/dev/sdc'/>
          <target dev='sdc' bus='scsi'/>
          <address type='drive' controller='0' bus='0' target='0' unit='0'/>
      </disk>
      ...
   </devices>
  </domain>
  ```

- Физический диск RDM
  
  Далее приведен пример монтирования диска SCSI **/dev/sdc** на хосте для предоставления его виртуальной машине в качестве физического неформатированного устройства:
  
  ```
  <domain type='kvm'>
   <devices>
      ...
      <controller type='scsi' model='virtio-scsi' index='0'/>
      <disk type='block' device='lun' rawio='yes'>
          <driver name='qemu' type='raw' cache='none' io='native'/>
          <source dev='/dev/sdc'/>
          <target dev='sdc' bus='scsi'/>
          <address type='drive' controller='0' bus='0' target='0' unit='0'/>
      </disk>
      ...
   </devices>
  </domain>
  ```

#### Изоляция и привязка процесса kworker

##### Обзор

Процесс kworker представляет собой поток для каждого отдельного процессора, реализованный ядром Linux. Он используется для выполнения запросов на рабочую очередь в системе. Потоки kworker конкурируют за ресурсы физического ядра с потоками vCPU, что приводит к нестабильной производительности виртуализированных служб. Чтобы обеспечить стабильную работу виртуальной машины и уменьшить в ней помехи от потоков kworker, можно привязать потоки kworker на хосте к определенному процессору.

##### Инструкции

Внесите изменения в файл **/sys/devices/virtual/workqueue/cpumask,** чтобы привязать задачи в рабочей очереди к процессору, заданному параметром **cpumasks**. Маски в параметре **cpumask** указываются в шестнадцатеричном формате. Например, если необходимо привязать поток kworker к процессорам CPU0–CPU7, выполните следующую команду, чтобы изменить маску на **ff**:

```
# echo ff > /sys/devices/virtual/workqueue/cpumask
```

#### Подкачка памяти методом HugePage

##### Обзор

Помимо традиционного метода подкачки виртуальной памяти размером 4 КБ, openEuler также поддерживает подкачку 2 МБ и 1 ГБ. Подкачка памяти методом больших страниц (HugePage) эффективно уменьшает частоту промахов попадания в буфер ассоциативной трансляции (TLB) и значительно повышает производительность сервисов, для работы которых требуется большой объем памяти. Для реализации подкачки памяти методом HugePage openEuler применяет две технологии.

- Технология статической подкачки HugePages
  
  В данном варианте перед загрузкой ОС хоста выделяется статический пул HugePage. При создании виртуальной машины необходимо изменить конфигурационный файл XML, указав, что память виртуальной машины выделяется из статического пула HugePage. Статический пул HugePage гарантирует, что вся память виртуальной машины будет представлена на хосте в виде сегментов HugePage для обеспечения неразрывности с физическими ресурсами. Однако это усложняет развертывание. После изменения размера страницы статического пула HugePage необходимо перезапустить хост, чтобы изменение вступило в силу. Размер статического пула HugePage может составлять 2 МБ или 1 ГБ.

- THP
  
  В условиях включенного режима «прозрачные страницы HugePage» (THP) машина автоматически выбирает доступные последовательно идущие страницы размером 2 МБ и автоматически разделяет и объединяет страницы HugePages при выделении сегментов памяти. Если нет доступных последовательно идущих страниц размером 2 МБ, виртуальная машина выбирает доступные страницы размером 64 КБ (архитектура AArch64) или 4 КБ (архитектура x86\_64) для выделения памяти. Пользователям не виден процесс THP, поэтому они просто используют метод подкачки HugePages с размером страниц 2 МБ, повышая скорость доступа к памяти.

Если в виртуальных машинах используются статические пулы HugePages, можно отключить функцию THP для уменьшения потребления ресурсов и обеспечения стабильной производительности виртуальной машины.

##### Инструкции

- Настройте статический пул HugePages.
  
  Перед созданием виртуальной машины внесите в конфигурационный файл XML настройки статического пула HugePage для виртуальной машины.
  
  ```
    <memoryBacking>
      <hugepages>
        <page size='1' unit='GiB'/>
      </hugepages>
    </memoryBacking>
  ```
  
  В приведенном выше примере содержимого XML-файла для виртуальной машины сконфигурирован статический пул HugePage размером 1 ГБ.
  
  ```
    <memoryBacking>
      <hugepages>
        <page size='2' unit='MiB'/>
      </hugepages>
    </memoryBacking>
  ```
  
  В приведенном выше примере содержимого XML-файла для виртуальной машины сконфигурирован статический пул HugePage размером 2 МБ.

- Настройте прозрачные страницы HugePage.
  
  Динамически включите THP через sysfs.
  
  ```
  # echo always > /sys/kernel/mm/transparent_hugepage/enabled
  ```
  
  Динамически отключите THP.
  
  ```
  # echo never > /sys/kernel/mm/transparent_hugepage/enabled
  ```

### Передовые методы обеспечения безопасности

#### Аутентификация libvirt

##### Обзор

Если пользователь прибегает к удаленному вызову службы libvirt без аутентификации, любая сторонняя программа, подключающаяся к сети хоста, может управлять виртуальными машинами через данный механизм. Такой подход представляет собой риск для безопасности. Для повышения безопасности системы openEuler предоставляет функцию аутентификации libvirt. То есть пользователи могут удаленно вызывать виртуальную машину через службу libvirt только после прохождения процедуры проверки подлинности идентификационных данных. Доступ к виртуальной машине могут получить только определенные пользователи, тем самым обеспечивается защита виртуальных машин в сети.

##### Включение функции аутентификации libvirt

По умолчанию функция удаленного вызова службы libvirt отключена на openEuler. Далее представлена процедура включения функции удаленного вызова и аутентификации службы libvirt.

1. Войдите в хост.

2. Измените конфигурационный файл службы libvirt **/etc/libvirt/libvirtd.conf**, включив функции удаленного вызова libvirt и аутентификации libvirt. Например, для включения удаленного вызова TCP на основе каркаса для добавления функций аутентификации и защиты данных в протоколы на основе соединений ( Simple Authentication and Security Layer; SASL) настройте параметры, ориентируясь на следующий пример:
   
   ```
   #Transport layer security protocol. The value 0 indicates that the protocol is disabled, and the value 1 indicates that the protocol is enabled. You can set the value as needed.
   listen_tls = 0
   #Enable the TCP remote invocation. To enable the libvirt remote invocation and libvirt authentication functions, set the value to 1.
   listen_tcp = 1
   #User-defined protocol configuration for TCP remote invocation. The following uses sasl as an example.
   auth_tcp = "sasl" 
   ```

3. Измените конфигурационный файл **/etc/sasl2/libvirt.conf**, внеся в него настройки механизма SASL и SASLDB.
   
   ```
   #Authentication mechanism of the SASL framework.
   mech_list: digest-md5
   #Database for storing usernames and passwords
   sasldb_path: /etc/libvirt/passwd.db
   ```

4. Добавьте пользователя для прохождения аутентификации SASL и установите пароль. Для примера взято имя пользователя **userName**. Команда выглядит следующим образом:
   
   ```
   # saslpasswd2 -a libvirt userName
   Password:
   Again (for verification):
   ```

5. Измените конфигурационный файл **/etc/sysconfig/libvirtd**, включив функцию прослушивания libvirt.
   
   ```
   LIBVIRTD_ARGS="--listen"
   ```

6. Перезапустите службу libvirtd, чтобы изменение вступило в силу.
   
   ```
   # systemctl restart libvirtd
   ```

7. Убедитесь, что функция аутентификации для удаленного вызова службы libvirt работает. Введите имя пользователя и пароль, следуя подсказкам. Если служба libvirt подключена, значит, функция успешно включена.
   
   ```
   # virsh -c qemu+tcp://192.168.0.1/system
   Please enter your authentication name: openeuler
   Please enter your password:
   Welcome to virsh, the virtualization interactive terminal.
   
   Type:  'help' for help with commands
          'quit' to quit
   
   virsh #
   ```

##### Управление SASL

Далее описывается процедура управления пользователями SASL.

- Запросите существующего пользователя из базы данных.
  
  ```
  # sasldblistusers2 -f /etc/libvirt/passwd.db
  user@localhost.localdomain: userPassword
  ```

- Удалите пользователя из базы данных.
  
  ```
  # saslpasswd2 -a libvirt -d user
  ```

#### qemu-ga

##### Обзор

Гостевой агент QEMU (qemu-ga) представляет собой демон, работающий в виртуальных машинах. Агент позволяет пользователям ОС хоста выполнять различные операции управления в гостевой операционной системе через внеполосные каналы, предоставляемые QEMU. Это операции с файлами (открытие, чтение, запись, закрытие, поиск и сброс на диск), внутреннее завершение работы, приостановка работы виртуальной машины (suspend-disk, suspend-ram, suspend-hybrid) и получение внутренней информации ВМ (включая информацию о памяти, процессоре, карте NIC и ОС).

В некоторых сценариях с высокими требованиями к безопасности агент qemu-ga предоставляет функцию черного списка, которая предотвращает утечку внутренней информации с виртуальных машин. Черный список можно использовать для избирательной защиты некоторых функций, предоставляемых агентом qemu-ga.

> ![](./public_sys-resources/icon-note.gif) ПРИМЕЧАНИЕ:  
Пакет установки агента qemu-ga: **qemu-guest-agent-**_xx_**.rpm**. По умолчанию он не установлен на openEuler. x.x.x означает номер версии.

##### Процедура

Чтобы добавить черный список агента qemu-ga, выполните следующие шаги:

1. Войдите в виртуальную машину и убедитесь, что служба qemu-guest-agent существует и работает.
   
   ```
   # systemctl status qemu-guest-agent |grep Active
      Active: active (running) since Wed 2018-03-28 08:17:33 CST; 9h ago
   ```

2. Запросите список команд **qemu-ga**, которые можно добавить в черный список:
   
   ```
   # qemu-ga --blacklist ?
   guest-sync-delimited
   guest-sync
   guest-ping
   guest-get-time
   guest-set-time
   guest-info
   ...
   ```

3. Сформируйте черный список. Добавьте команды, которые необходимо скрыть в списке **--blacklist,** в файл **/usr/lib/systemd/system/qemu-guest-agent.service**. Для разделения команд используются пробелы. Например, чтобы добавить команды **guest-file-open** и **guest-file-close** в черный список, внесите их в файл настройки, ориентируясь на следующий пример:
   
   ```
   [Service]
   ExecStart=-/usr/bin/qemu-ga \
         --blacklist=guest-file-open guest-file-close
   ```

4. Перезапустите службу qemu-guest-agent.
   
   ```
   # systemctl daemon-reload
   # systemctl restart qemu-guest-agent
   ```

5. Убедитесь, что функция черного списка агента qemu-ga вступила в силу на данной виртуальной машине, то есть параметр **--blacklist** корректно сконфигурирован для процесса qemu-ga.
   
   ```
   # ps -ef|grep qemu-ga|grep -E "blacklist=|b="
   root       727     1  0 08:17 ?        00:00:00 /usr/bin/qemu-ga --method=virtio-serial --path=/dev/virtio-ports/org.qemu.guest_agent.0 --blacklist=guest-file-open guest-file-close guest-file-read guest-file-write guest-file-seek guest-file-flush -F/etc/qemu-ga/fsfreeze-hook
   ```
   
   > ![](./public_sys-resources/icon-note.gif) ПРИМЕЧАНИЕ:  
Для получения дополнительной информации об агенте qemu-ga перейдите на страницу [https://wiki.qemu.org/Features/GuestAgent](https://wiki.qemu.org/Features/GuestAgent).

#### Защита sVirt

##### Обзор

В среде виртуализации, которая использует только лишь политику избирательного контроля доступа (Discretionary Access Control; DAC), работающие на хостах вредоносные виртуальные машины могут атаковать гипервизор или другие виртуальные машины. Чтобы повысить безопасность в сценариях виртуализации, openEuler использует для защиты технологию защиты sVirt, основанную на SELinux. Технология применяется в сценариях виртуализации KVM. Виртуальная машина рассматривается в операционной системе хоста как общий процесс. В гипервизоре механизм sVirt помечает процессы QEMU, связанные с соответствующими виртуальными машинами, специальными метками SELinux. Помимо типов различных файлов и процессов виртуализации, для маркировки виртуальных машин используются категории. Каждая виртуальная машина может получить доступ только к файлам одной категории. Эта мера не допускает доступ к файлам и устройствам на неразрешенных хостах или других виртуальных машинах, тем самым предотвращает аварийное завершение работы виртуальной машины и повышает безопасность хоста и ВМ.

##### Включение защиты sVirt

1. Включите SELinux на хосте.
   
   1. Войдите в хост.
   
   2. Включите функцию SELinux на хосте.
      
      1. Измените в файле **grub.cfg** параметр запуска системы **selinux**, задав значение **1**.
         
         ```
         selinux=1
         ```
      
      2. В файле **/etc/selinux/config** задайте параметру **SELINUX** значение **enforcing**.
         
         ```
         SELINUX=enforcing
         ```
   
   3. Перезапустите хост.
      
      ```
      # reboot
      ```

2. Создайте виртуальную машину, в которой включена функция sVirt.
   
   1. Добавьте следующую информацию в конфигурационный файл виртуальной машины:
      
      ```
      <seclabel type='dynamic' model='selinux' relabel='yes'/>
      ```
      
      Или удостоверьтесь, что в данном файле настроена следующая конфигурация:
      
      ```
      <seclabel type='none' model='selinux'/>
      ```
   
   2. Создайте виртуальную машину.
      
      ```
      # virsh define openEulerVM.xml
      ```

3. Убедитесь, что функция sVirt включена.
   
   Выполните следующую команду, чтобы убедиться, что защита sVirt для процесса QEMU текущей виртуальной машины включена. Наличие информации **svirt\_t:s0:c** свидетельствует о том, что защита sVirt включена.
   
   ```
   # ps -eZ|grep qemu |grep "svirt_t:s0:c"
   system_u:system_r:svirt_t:s0:c200,c947 11359 ? 00:03:59 qemu-kvm
   system_u:system_r:svirt_t:s0:c427,c670 13790 ? 19:02:07 qemu-kvm
   ```